# 파이프라인

로그스태시의 가장 중요한 부분은 파이프라인이다.  
파이프라인은 데이터를 입력받아 실시간으로 변경하고 이를 다른 시스템에 전달하는 역할을 한다.   

파이프라인은 `입력`, `필터`, `출력`, 3단계로 나뉘어진다.   
* 입력 : 소스로부터 데이터를 받아들이는 모듈
* 필터 : 입력으로 들어오는 데이터를 원하는 형태로 가공하는 모듈 
* 출력 : 데이터를 외부로 전달하는 모듈 

```
/bin/logstash -e "input { stdin{} } output { stdout } }"
> hello world!

"@version" => "1",
"host" => "컴퓨터 HOST"
"message" => "hello world\r"
"@timestamp" => 2021-12-24T22:24:40.352Z
```
console I/O 기반의 로그 스태시를 실행하고 명령어를 입력하면 위와 같은 결과가 출력된다. 
  
로그스태시는 JSON 형태로 데이터를 출력하는데,     
@version 이나 @timestamp 는 로그 스태시가 만든 필드로   
사용자가 만든 필드와 충돌이 날 것을 대비해 앞에 @ 기호가 붙어있다.  
 
`@` 기호는 로그스태시에 의해 생성된 필드,  
붙지않은 필드는 수집을 통해 얻어진 정보라고 이해하자.   

```
input {
    { 입력 플러그인 } 
}

filter {
    { 필터 플러그인 }
}

output {
    { 출력 플러그인 }
}
```
파이프라인을 구성하는 기본 템플릿은 위와 같다.    
용도나 형태에 따라 이미 만들어진 다양한 플러그인이 있기 때문에     
필요한 기능을 지원하는 플러그인을 검색하여 템플릿에 추가하면 된다.    

## 입력 

소스 원본으로부터 데이터를 입력받는 단계다.   
직접 대상에 접근해 읽어들이는 경우도 있지만, 서버를 열어놓고 받아들이는 형태의 구성도 가능하다.   

[이미지](이미지)

로그 스태시는 다양한 형태의 데이털르 인식할 수 있고, 이를 쉽게 처리하기 위해 다양한 입력 플러그인들이 존재한다.   


|입력 플러그인|설명|
|---------|---|
|file|리눅스의 tail -f 명령처럼 파일을 스트리밍하며 이벤트를 읽어들인다.|
|syslog|네트워크를 통해 전달되는 시스로그를 수신한다|
|kafka|카프카의 토픽에서 데이터를 읽어 들인다|
|jdbc|JDBC 드라이버로 지정한 일정마다 쿼리를 실행해 결과를 읽어들인다| 

위 테이블은 자주 사용하는 플러그인 목록이다.   
파일 플러그인은 시스템의 특정 파일을 읽어올 수 있도록 구현된 플러그인으로, 예시에서 사용해볼 예정이다.    

**logstash-test.conf**
```
input {
    file {
        path => "C:/elasticsearch-7.10.1/logs/elasticsearch.log
        start_position => "begining"
    }
    
    output {
        stdout { }
    }
}
```
**logstash-test.conf**를 로그스태시가 설치된 config 파일에 둔다.    
파이프라인 설정 위치 역시 필수는 아니지만, 가능하면 프로젝트끼리 묶어두는 것이 소스 관리 차원에서 좋다.   
 
이제 파일에 로그가 쌓이면 실시간으로 elasticsearch 파일의 변경을 감지해 읽어들인다.      
start_position 은 최초 파일을 발견했을 때 파일을 읽을 위치로, 설정에 따라 다르게 동작시킬 수 있다.  

```
./bin/logstash -f ./config/logstash-test.conf
```

실행 결과중에, @version, path, host, @timestamp 같은 필드는 접어두고 message 필드에만 집중하자   
message에 적힌 구문을 분석해 의미 있는 데이터로 변환하는 역할을 필터 플러그인이 한다.     

## 필터
> 필터는 로그스태시가 가지고 있는 기능 중에서 가장 중요한 기능이라고 볼 수 있다.    
   
필터는 입력받은 데이터를 의미있는 데이터로 구조화하는 역할을 한다.    
필수 구성 요소는 아니지만, 필터 없는 파이프라인은 그 기능을 온전히 발휘하기 힘들다.  

로그스태시 필터는 비정형 데이터를 정형화하고 데이터 분석을 위한 구조를 잡아준다.   
비츠나 키바나 등에서 입력받은 데이터를 로그스태시 필터를 이용해 필요한 정보만     
손쉽게 추출하거나 형태를 변환하고 부족한 정보는 추가하는 등 전반적인 데이터 정제/가공 작업을 수행할 수 있다.   
  
이렇게 정형화된 데이터는, 엘라스틱 서치나 아마존 S3와 같은 스토리지에 전송되어 분석등의 용도로 활용된다.    
로그스태시, 그중에서 필터는 데이터를 정형화하고 사용자가 필요한 데이터 형태로 가공하는데 핵심적인 역할을 한다.    

**필터 역시 플러그인 형태이며,** 입력과 비슷하게 다양한 필터 플러그인이 존재한다.   
전체 플러그인은 온라인 문서를 확인하고, 여기서는 자주 사용하는 필터만 정리해본다.  

|필터 플러그인|설명|
|---------|---|
|grok|grok 패턴을 사용해 메시지를 구조화된 형태로 분석한다.<br>grok 패턴은 일반적인 정규식과 유사하나,<br>추가적으로 미리 정의된 패턴이나 필드 이름설정, 데이터 타입 정의등을 도와준다.|
|dissect|간단한 패턴을 사용해, 메시지를 구조화된 형태로 분석한다.<br>정규식을 사용하지 않아 grok에 비해 자유도는 조금 떨어지지만, 빠른처리가 가능하다|
|mutate|필드명을 변경하거나 문자열 처리등 일반적인 가공 함수들을 제공한다|
|date|문자열을 지정한 패턴의 날짜형으로 분석한다|

예제를 진행하기 위한 파일을 하나 만든다.   

**filter-example.log**
```
[2022-08-03 16:00] [ID1] 127.0.0.1 9500 [INFO] - connected.
[2022-08-03 16:02:45]   [ID2] 172.0.0.1 1070 [warn] - busy server.
```

메모장을 열고 위와 같은 로그 파일을 만든다.     
그리고, `logstash-test.conf` 파일을 다음과 같이 수정한다.   
   
**logstash-test.conf**
```
input {
    file {
        path => "/Users/kimwoojae/IdeaProjects/poc/elastic/logstash-7.15.0/config/filter-example.log"
        start_position => "beginning"
        sincedb_path => "nul"
    }
}

output {
  stdout { }
}
``` 
`sincedb_path` 라는 설정이 추가되었다.     
`sincedb_path` 라는 설정은 로그를 어디까지 읽었는지 파일에 저장하는 기능이다.  
     
`start_position => "beginning"`의 경우 로그 파일을 처음부터 읽는 설정이다.   
다만, 기존에 읽었다면 추가로 읽는 것은 리소스 낭비이므로 로그스태시에서는 디폴트 값으로 `sincedb_path`를 사용한다.      
`sincedb_path`의 기본 설정은 로그스태시의 data 폴더의 `data/plugins/inputs/file` 위치에 since_db 파일을 생성한다.     
이 설정을 `nul`로 처리했으니, 매번 로그 파일을 처음부터 읽는다고 생각하면 된다.   

```
./bin/logstash -f ./config/logstash-test.conf --config.reload.automatic
```
이후 실제로 로그스태시를 실행하면 아래와 같은 결과가 나온다.   
   
```
{
      "@version" => "1",
          "host" => "Kimui-MacBookPro.local",
    "@timestamp" => 2022-08-07T08:17:03.122Z,
       "message" => "[2022-08-03 16:00] [ID1] 127.0.0.1 9500 [INFO] - connected",
          "path" => "/Users/kimwoojae/IdeaProjects/poc/elastic/logstash-7.15.0/config/filter-example.log"
}
{
      "@version" => "1",
          "host" => "Kimui-MacBookPro.local",
    "@timestamp" => 2022-08-07T08:17:03.155Z,
       "message" => "[2022-08-03 16:02:45] [ID2] 172.0.0.1 1070 [warn] - busy server.",
          "path" => "/Users/kimwoojae/IdeaProjects/poc/elastic/logstash-7.15.0/config/filter-example.log"
}
```

### 문자열 자르기 
데이터나 로그는 대부분 길이가 길기 때문에 우리가 원하는 형태로 분리해야한다.    
필터 플러그인중 문자열을 분리하는 플러그인이 있으므로 이를 통해 원하는 형태로 분리작업을 진행해보자.     

```
input {
    file {
        path => "/Users/kimwoojae/IdeaProjects/poc/elastic/logstash-7.15.0/config/filter-example.log"
        start_position => "beginning"
        sincedb_path => "nul"
    }
}

filter {
    mutate {
        split => { "message" => " "}
    }
}

output {
  stdout { }
}
```
mutate 플러그인은 필드를 변형하는 다양한 기능들을 제공하고 있다.      
필드 이름을 바꾸거나, 변경하거나 삭제하는 작업등을 할 수 있다.      
mutate는 내부 옵션이 다양한데 split 도 여러 옵션 중 하나이다.    

|mutate 옵션|설명|
|----------|---|
|split|쉼표, 같은 구분자를 기준으로 문자열을 배열로 나눈다.|
|rename|필드 이름을 바꾼다|
|replace|해당 필드값을 특정 값으로 바꾼다|
|uppercase|문자를 대문자로 변경한다|
|lowercase|문자를 소문자로 변경한다|
|join|배열을 쉼표, 같은 구분자로 연결해 하나의 문자열로 합친다|
|gsub|정규식이 일치하는 항목을 다른 문자열로 대체한다|
|merge|특정 필드를 다른 필드에 포함시킨다|   
|coerce|null인 필드값에 기본값을 넣어준다|   
|strip|필드값의 좌우 공백을 제거한다|     

mutate 내부 옵션에는 순서가 정해져있는데 다음과 같다.

1. coerce 
2. rename
3. update
4. replace
5. convert 
6. gsub
7. uppercase
8. capitalize
9. lowercase
10. strip
11. remove
12. split
13. join
14. merge
15. copy

이제 위 스크립트를 기반으로 로그스태시를 실행하면 아래와 같은 결과가 나온다.  

```
{
      "@version" => "1",
          "host" => "Kimui-MacBookPro.local",
          "path" => "/Users/kimwoojae/IdeaProjects/poc/elastic/logstash-7.15.0/config/filter-example.log",
       "message" => [
        [0] "[2022-08-03",
        [1] "16:02:45]",
        [2] "[ID2]",
        [3] "172.0.0.1",
        [4] "1070",
        [5] "[warn]",
        [6] "-",
        [7] "busy",
        [8] "server."
    ],
    "@timestamp" => 2022-08-07T08:27:35.705Z
}
{
      "@version" => "1",
          "host" => "Kimui-MacBookPro.local",
          "path" => "/Users/kimwoojae/IdeaProjects/poc/elastic/logstash-7.15.0/config/filter-example.log",
       "message" => [
        [0] "[2022-08-03",
        [1] "16:00]",
        [2] "[ID1]",
        [3] "127.0.0.1",
        [4] "9500",
        [5] "[INFO]",
        [6] "-",
        [7] "connected"
    ],
    "@timestamp" => 2022-08-07T08:27:35.677Z
}
```
message 필드 문자열이 공백을 기준으로 구분되어, 배열 형태의 데이터가 되었다.  
구분된 문자열은 `필드명 [숫자]`와 같이 접근할 수 있디.  

```
input {
    file {
        path => "/Users/kimwoojae/IdeaProjects/poc/elastic/logstash-7.15.0/config/filter-example.log"
        start_position => "beginning"
        sincedb_path => "nul"
    }
}

filter {
    mutate {
        split => { "message" => " "}
        add_field => { "id" => "%{[message][2]}" }
        remove_field => "message"
    }
}

output {
  stdout { }
}
```
add_field 옵션은 새로운 필드를 만드는 옵션이다.       
remove_field 옵션은 필드를 삭제하는 옵션이다.        
id 라는 필드를 생성하고, 앞서 만든 message 필드의 2번째 요소를 가져와 필드로 만들고 있다.   

|공통 옵션|설명|
|-------|---|
|add_field|새로운 필드를 추가할 수 있다|
|add_tag|성공한 이벤트에 태그를 추가할 수 있다|
|enable_metric|메트릭 로깅을 활성화하거나 비활성화 할 수 있다.<br>기본적으로 활성화되어 있으며,<br>수집된 데이터는 로그스태시 모니터링에서 해당 필터의 성능을 분석할때 사용한다|
|id|플러그인의 아이디를 설정한다<br>모니터링시 아이디를 이용해 특정 플러그인을 쉽게 찾을 수 있다|
|remove_field|필드를 삭제할 수 있다|
|remove_tag|성공한 이벤트에 붙은 태그를 삭제할 수 있다|
    
로그스태시를 다시 실행해보면 아래와 같은 결과가 나온다   

```
{
          "path" => "/Users/kimwoojae/IdeaProjects/poc/elastic/logstash-7.15.0/config/filter-example.log",
            "id" => "[ID2]",
      "@version" => "1",
    "@timestamp" => 2022-08-07T08:49:05.049Z,
          "host" => "Kimui-MacBookPro.local"
}
{
          "path" => "/Users/kimwoojae/IdeaProjects/poc/elastic/logstash-7.15.0/config/filter-example.log",
            "id" => "[ID1]",
      "@version" => "1",
    "@timestamp" => 2022-08-07T08:49:04.999Z,
          "host" => "Kimui-MacBookPro.local"
}
```
필터 플러그인은 순서대로 동작하는데 먼저 split 에 의해 message 필드의 문자열이 공백을 기준으로 분리되었다.   
다음으로 id 필드를 추가하고 필드값은 `[message][2]` 를 사용한다.   
마지막으로 message 필드가 삭제되었다.   

로그스태시는 이처럼 사용하기 쉬우면서 필터들을 통해 강력한 문자열 변환이 가능함을 알 수 있다.  
참고로 split 플러그인도 존재하는데 mutate에 속하지 않은 별개의 플러그인이고  
단일 문서를 복수의 문서로 나눠주는 역할을 한다.(혼동하지 말자)   

### dissect 이용한 문자열 파싱 

mutate 플러그인의 split 옵션을 이요했지만, 하나의 구분자만 이용해서 데이터를 나눠야한다.  
dissect 플러그인은 패턴을 이용해 문자올열을 분석하고 주요 정보를 필드로 추출하는 기능을 수행한다.  

```
input {
    file {
        path => "/Users/kimwoojae/IdeaProjects/poc/elastic/logstash-7.15.0/config/filter-example.log"
        start_position => "beginning"
        sincedb_path => "nul"
    }
}

filter {
    dissect {
        mapping => { "message" => "[%{timestamp}] [%{id}] %{ip} %{port} [%{level}] - %{message}." }
    }
}

output {
  stdout { }
}
```
dissect 를 이용해서 특정 패턴을 가진 로그를 분석한다.      
위 코드는 앞서 정의한 log 형태가 맞다면 필터링에 성공한다.      

```
{
          "port" => "9500",
          "host" => "KimuiMacBookPro",
     "timestamp" => "2022-08-03 16:00",
         "level" => "INFO",
       "message" => "connected",
            "id" => "ID1",
      "@version" => "1",
    "@timestamp" => 2022-08-07T10:22:16.824Z,
          "path" => "/Users/kimwoojae/IdeaProjects/poc/elastic/logstash-7.15.0/config/filter-example.log",
            "ip" => "127.0.0.1"
}
{
          "host" => "KimuiMacBookPro",
       "message" => "[2022-08-03 16:02:45]   [ID2] 172.0.0.1 1070 [warn] - busy server.",
      "@version" => "1",
    "@timestamp" => 2022-08-07T10:22:16.848Z,
          "path" => "/Users/kimwoojae/IdeaProjects/poc/elastic/logstash-7.15.0/config/filter-example.log",
          "tags" => [
        [0] "_dissectfailure"
    ]
}
```
실제 결과는 위와 같다.          
dissect 플러그인의 mapping 옵션에 구분자 형태를 정의하고 필드를 구분한다.        
`%{필드명}`으로 작성하몀ㄴ 중괄호(`{}`)안의 필드명으로 새로운 필드가 만들어진다.     
`%{필드명}`외의 문자열들은 모두 구분자 역할을 한다.    

```
[timestamp] [id] ip port [level] - msg
```
`[ID1]`의 경우 모든 형식이 맞지만 `[ID2]`의 경우 중간에 띄어쓰기가 여럿 중첩되어 실패했다.  
이를 해결하기 위해 아래와 같이 conf 파일을 수정한다.  

```
input {
    file {
        path => "/Users/kimwoojae/IdeaProjects/poc/elastic/logstash-7.15.0/config/filter-example.log"
        start_position => "beginning"
        sincedb_path => "nul"
    }
}

filter {
    dissect {
        mapping => { "message" => "[%{timestamp}]%{?->}[%{id}] %{ip} %{port} [%{level}] - %{message}." }
    }
}

output {
  stdout { }
}
```
```
{
          "path" => "/Users/kimwoojae/IdeaProjects/poc/elastic/logstash-7.15.0/config/filter-example.log",
          "host" => "KimuiMacBookPro",
       "message" => "connected",
    "@timestamp" => 2022-08-07T10:34:17.319Z,
            "id" => "ID1",
            "ip" => "127.0.0.1",
         "level" => "INFO",
     "timestamp" => "2022-08-03 16:00",
      "@version" => "1",
          "port" => "9500"
}
{
          "path" => "/Users/kimwoojae/IdeaProjects/poc/elastic/logstash-7.15.0/config/filter-example.log",
          "host" => "KimuiMacBookPro",
       "message" => "busy server",
    "@timestamp" => 2022-08-07T10:34:17.347Z,
            "id" => "ID2",
            "ip" => "172.0.0.1",
         "level" => "warn",
     "timestamp" => "2022-08-03 16:02:45",
      "@version" => "1",
          "port" => "1070"
}
```
`%{?->}`은 여럿 공백도 하나의 공백으로 취급해주는 패턴이다.     
이를 통해 로그가 정상적으로 나오는 것을 확인할 수 있다.    


