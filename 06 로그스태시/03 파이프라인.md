# 파이프라인

로그스태시의 가장 중요한 부분은 파이프라인이다.  
파이프라인은 데이터를 입력받아 실시간으로 변경하고 이를 다른 시스템에 전달하는 역할을 한다.   

파이프라인은 `입력`, `필터`, `출력`, 3단계로 나뉘어진다.   
* 입력 : 소스로부터 데이터를 받아들이는 모듈
* 필터 : 입력으로 들어오는 데이터를 원하는 형태로 가공하는 모듈 
* 출력 : 데이터를 외부로 전달하는 모듈 

```
/bin/logstash -e "input { stdin{} } output { stdout } }"
> hello world!

"@version" => "1",
"host" => "컴퓨터 HOST"
"message" => "hello world\r"
"@timestamp" => 2021-12-24T22:24:40.352Z
```
console I/O 기반의 로그 스태시를 실행하고 명령어를 입력하면 위와 같은 결과가 출력된다. 
  
로그스태시는 JSON 형태로 데이터를 출력하는데,     
@version 이나 @timestamp 는 로그 스태시가 만든 필드로   
사용자가 만든 필드와 충돌이 날 것을 대비해 앞에 @ 기호가 붙어있다.  
 
`@` 기호는 로그스태시에 의해 생성된 필드,  
붙지않은 필드는 수집을 통해 얻어진 정보라고 이해하자.   

```
input {
    { 입력 플러그인 } 
}

filter {
    { 필터 플러그인 }
}

output {
    { 출력 플러그인 }
}
```
파이프라인을 구성하는 기본 템플릿은 위와 같다.    
용도나 형태에 따라 이미 만들어진 다양한 플러그인이 있기 때문에     
필요한 기능을 지원하는 플러그인을 검색하여 템플릿에 추가하면 된다.    

## 입력 

소스 원본으로부터 데이터를 입력받는 단계다.   
직접 대상에 접근해 읽어들이는 경우도 있지만, 서버를 열어놓고 받아들이는 형태의 구성도 가능하다.   

[이미지](이미지)

로그 스태시는 다양한 형태의 데이털르 인식할 수 있고, 이를 쉽게 처리하기 위해 다양한 입력 플러그인들이 존재한다.   


|입력 플러그인|설명|
|---------|---|
|file|리눅스의 tail -f 명령처럼 파일을 스트리밍하며 이벤트를 읽어들인다.|
|syslog|네트워크를 통해 전달되는 시스로그를 수신한다|
|kafka|카프카의 토픽에서 데이터를 읽어 들인다|
|jdbc|JDBC 드라이버로 지정한 일정마다 쿼리를 실행해 결과를 읽어들인다| 

위 테이블은 자주 사용하는 플러그인 목록이다.   
파일 플러그인은 시스템의 특정 파일을 읽어올 수 있도록 구현된 플러그인으로, 예시에서 사용해볼 예정이다.    

**logstash-test.conf**
```
input {
    file {
        path => "C:/elasticsearch-7.10.1/logs/elasticsearch.log
        start_position => "begining"
    }
    
    output {
        stdout { }
    }
}
```
**logstash-test.conf**를 로그스태시가 설치된 config 파일에 둔다.    
파이프라인 설정 위치 역시 필수는 아니지만, 가능하면 프로젝트끼리 묶어두는 것이 소스 관리 차원에서 좋다.   
 
이제 파일에 로그가 쌓이면 실시간으로 elasticsearch 파일의 변경을 감지해 읽어들인다.      
start_position 은 최초 파일을 발견했을 때 파일을 읽을 위치로, 설정에 따라 다르게 동작시킬 수 있다.  

```
./bin/logstash -f ./config/logstash-test.conf
```

실행 결과중에, @version, path, host, @timestamp 같은 필드는 접어두고 message 필드에만 집중하자   
message에 적힌 구문을 분석해 의미 있는 데이터로 변환하는 역할을 필터 플러그인이 한다.     

## 필터
> 필터는 로그스태시가 가지고 있는 기능 중에서 가장 중요한 기능이라고 볼 수 있다.    
   
필터는 입력받은 데이터를 의미있는 데이터로 구조화하는 역할을 한다.    
필수 구성 요소는 아니지만, 필터 없는 파이프라인은 그 기능을 온전히 발휘하기 힘들다.  

로그스태시 필터는 비정형 데이터를 정형화하고 데이터 분석을 위한 구조를 잡아준다.   
비츠나 키바나 등에서 입력받은 데이터를 로그스태시 필터를 이용해 필요한 정보만     
손쉽게 추출하거나 형태를 변환하고 부족한 정보는 추가하는 등 전반적인 데이터 정제/가공 작업을 수행할 수 있다.   
  
이렇게 정형화된 데이터는, 엘라스틱 서치나 아마존 S3와 같은 스토리지에 전송되어 분석등의 용도로 활용된다.    
로그스태시, 그중에서 필터는 데이터를 정형화하고 사용자가 필요한 데이터 형태로 가공하는데 핵심적인 역할을 한다.    

**필터 역시 플러그인 형태이며,** 입력과 비슷하게 다양한 필터 플러그인이 존재한다.   
전체 플러그인은 온라인 문서를 확인하고, 여기서는 자주 사용하는 필터만 정리해본다.  

|필터 플러그인|설명|
|---------|---|
|grok|grok 패턴을 사용해 메시지를 구조화된 형태로 분석한다.<br>grok 패턴은 일반적인 정규식과 유사하나,<br>추가적으로 미리 정의된 패턴이나 필드 이름설정, 데이터 타입 정의등을 도와준다.|
|dissect|간단한 패턴을 사용해, 메시지를 구조화된 형태로 분석한다.<br>정규식을 사용하지 않아 grok에 비해 자유도는 조금 떨어지지만, 빠른처리가 가능하다|
|mutate|필드명을 변경하거나 문자열 처리등 일반적인 가공 함수들을 제공한다|
|date|문자열을 지정한 패턴의 날짜형으로 분석한다|

예제를 진행하기 위한 파일을 하나 만든다.   

**filter-example.log**
```
C:/logstash-7.10.1>copy.con ./config/filter-example.log
[2022-08-03 16:00][ID1] ...
[2022-08-03 16:00][ID2] ...
```

메모장을 열고 위와 같은 파일을 만든다.   

**logstash-test.conf**
```
input {
    file {
        path => "C:/logstash-7.10.1/config/filter-example.log"
        start_position =? "begining"
        sincedb_path => "null"
    }
}

output {
    stdout { }
}
```
